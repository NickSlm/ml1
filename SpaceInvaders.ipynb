{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fa653d-86b3-4b2e-b6f7-447df96f441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434decc1-446f-46c1-8505-b8e71902b981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 15:25:45.061419: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-19 15:25:45.882111: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/nick/miniconda3/envs/tf_env/lib/\n",
      "2024-12-19 15:25:45.882236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nick/miniconda3/envs/tf_env/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/nick/miniconda3/envs/tf_env/lib/\n",
      "2024-12-19 15:25:45.882245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import gymnasium\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import ale_py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "tf.keras.utils.disable_interactive_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba14a74b-f78c-4ac5-92b4-6a80aecf15d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f65bf5f7-ad94-43e3-bc26-4a8f0c71b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.show()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943bf12c-7385-457f-bfe1-f700c97b3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariPreprocessingFrameSkip(AtariPreprocessing):\n",
    "    def reset(self, **kwargs):\n",
    "        obs, reset_info = super().reset(**kwargs)\n",
    "        for _ in range(40):\n",
    "            super().step(0)\n",
    "        return obs, reset_info\n",
    "    def step(self, action):\n",
    "        self.lives_before_action = self.ale.lives()\n",
    "        obs, rewards, terminated, truncated, info = super().step(action)\n",
    "        done = terminated or truncated\n",
    "        if not done and self.ale.lives() < self.lives_before_action:\n",
    "            for _ in range(40):\n",
    "                super().step(0)\n",
    "        return obs, rewards, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2523371-8b41-485f-a7ec-4420d7d3f52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "env = AtariPreprocessingFrameSkip(env)\n",
    "env = FrameStackObservation(env, stack_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce238e06-1234-4ee8-aa3d-d47aef0e014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = env.observation_space.shape\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "q_net = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=n_inputs),\n",
    "    keras.layers.Lambda(lambda obs: tf.cast(obs, tf.float32) / 255.),\n",
    "    keras.layers.Conv2D(32, kernel_size=(8,8), strides=4, activation=\"relu\", data_format=\"channels_first\"),\n",
    "    keras.layers.Conv2D(64, kernel_size=(4,4), strides=2, activation=\"relu\", data_format=\"channels_first\"),\n",
    "    keras.layers.Conv2D(64, kernel_size=(3,3), strides=1, activation=\"relu\", data_format=\"channels_first\"),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation=\"relu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "    \n",
    "])\n",
    "\n",
    "target_net = keras.models.clone_model(q_net)\n",
    "target_net.set_weights(q_net.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b70f2356-5cdd-415d-9654-e268b300877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self, env, q_net, target_net, discount_rate, replay_buffer, loss_fn, optimizer):\n",
    "        self.env = env\n",
    "        self.q_net = q_net\n",
    "        self.target_net = target_net\n",
    "        self.discount_rate = discount_rate\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "                        initial_learning_rate=1.0,\n",
    "                        decay_steps=20000,\n",
    "                        end_learning_rate=0.01\n",
    "                        )\n",
    "    \n",
    "    def greedy_policy(self, state, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "        q_values = self.q_net.predict(state[np.newaxis])\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def initialize_buffer(self, n_max_steps):\n",
    "        state, _ = self.env.reset()\n",
    "        for _ in range(n_max_steps):\n",
    "            action = self.env.action_space.sample()\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "    \n",
    "    def collect_step(self, state, iteration):\n",
    "        epsilon = self.epsilon_fn(iteration)\n",
    "        action = self.greedy_policy(state, epsilon)\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def train_step(self, batch_size):\n",
    "        random_indices = np.random.randint(0, len(self.replay_buffer), batch_size)\n",
    "        samples = [self.replay_buffer[index] for index in random_indices]\n",
    "        states, actions, rewards, next_states, dones = [np.array([sample[field_index] \n",
    "                                                                  for sample in samples]) \n",
    "                                                        for field_index in range(5)]\n",
    "\n",
    "        q_next_values = self.q_net.predict(next_states)\n",
    "        q_next_mask = tf.one_hot(np.argmax(q_next_values, axis=1), n_outputs).numpy()\n",
    "\n",
    "        t_q_values = self.target_net.predict(next_states)\n",
    "        t_q_max_values = (t_q_values * q_next_mask).sum(axis=1)\n",
    "        t_q_max_values = (rewards + t_q_max_values * self.discount_rate * (1 - dones))\n",
    "        t_q_max_values.reshape(-1, 1)\n",
    "\n",
    "        q_mask = tf.one_hot(actions, n_outputs)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.q_net(states)\n",
    "            q_values = tf.reduce_sum(q_values * q_mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(self.loss_fn(t_q_max_values, q_values))\n",
    "\n",
    "        gradients = tape.gradient(loss, self.q_net.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.q_net.trainable_variables))\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae6203b4-a78f-44a1-82bd-7ec9e4dac053",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50000\n",
    "n_max_steps = 1000\n",
    "discount_rate = 0.99\n",
    "replay_buffer = deque(maxlen=50000)\n",
    "loss_fn = keras.losses.Huber()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0102a2-8395-4cc7-8512-ffa2bfc43797",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DqnAgent(\n",
    "    env=env,\n",
    "    q_net=q_net,\n",
    "    target_net=target_net,\n",
    "    discount_rate=discount_rate,\n",
    "    replay_buffer=replay_buffer,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b4ee30-6550-490f-8bb8-2bb52eb1a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.initialize_buffer(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ecc98-12df-4057-ae9c-c1535a176fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\tEpisode reward: 0WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f1ee00698b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f1ee00698b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f1ee00698b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f1ee00698b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20490\tEpisode reward: 90.00"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs\")\n",
    "\n",
    "\n",
    "\n",
    "total_rewards = 0\n",
    "episode_rewards = [0]\n",
    "state, _ = env.reset()\n",
    "for iteration in range(n_iterations):\n",
    "    keras.backend.clear_session()\n",
    "    for step in range(4):\n",
    "        state, reward, done = dqn_agent.collect_step(state, iteration)\n",
    "        total_rewards += reward\n",
    "        if done:\n",
    "            episode_rewards.append(total_rewards)\n",
    "            total_rewards = 0\n",
    "            state,_ = env.reset()\n",
    "    print(f\"\\rIteration: {iteration}\\tEpisode reward: {episode_rewards[-1]}\", end=\"\")\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(\"Reward\", episode_rewards[-1], step=len(episode_rewards))\n",
    "    dqn_agent.train_step(batch_size=32)\n",
    "    if iteration % 2000 == 0:\n",
    "        dqn_agent.target_net.set_weights(dqn_agent.q_net.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30bfb6a-7ca6-453c-8eaf-c0056a42f23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
